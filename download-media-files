#!/usr/bin/env python3

import argparse
import json
import logging
import sys
import time
from pathlib import Path
from typing import IO, Any, Iterator
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

import requests

MediaObject = dict[str, Any]


def read_media_objects(f: IO) -> Iterator[MediaObject]:
    for line in f:
        yield json.loads(line)


def download_media_file(url: str, path: Path):
    if path.is_file():
        logger.info(f'Media file {url} found at {path}. Skipping download.')
        return
    logger.info(f'Downloading media file {url} to {path}...')
    r = requests.get(url)
    r.raise_for_status()
    path.write_bytes(r.content)
    time.sleep(3)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        'media_objects_file', type=argparse.FileType('r'), default=sys.stdin
    )
    parser.add_argument('media_files_dir', type=Path)
    parser.add_argument(
        '-v', '--verbose', action='store_true', help='Enable debugging output'
    )
    args = parser.parse_args()
    if args.verbose:
        logging.basicConfig(
            stream=sys.stderr, level=logging.INFO, format='%(message)s'
        )
    args.media_files_dir.mkdir(parents=True, exist_ok=True)

    for media_object in read_media_objects(args.media_objects_file):
        includes = media_object.get('includes')
        if not includes:
            continue
        media_files = includes.get('media')
        if not media_files:
            continue
        for media_file in media_files:
            url = media_file.get('url')
            if not url:
                continue
            u = urlparse(url)
            filename = u.path.split('/')[-1]
            download_media_file(url, args.media_files_dir / filename)

    logger.info('Done downloading media files')


if __name__ == '__main__':
    main()
